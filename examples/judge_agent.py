"""
Judge Agent: Critical evaluation of agent outputs with improvement suggestions.

This agent reviews the outputs from agents 1-5 and provides:
- Critical analysis of correctness
- Quality assessment
- Improvement suggestions
- Best practices recommendations
"""

import asyncio
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from pydantic_ai import Agent, ModelSettings

from app.models import judge_model


# ============================================================================
# JUDGMENT MODELS
# ============================================================================

class AgentOutputReview(BaseModel):
    """Review of a single agent's output."""
    agent_name: str = Field(description="Name of the agent being reviewed")
    correctness_score: float = Field(description="Correctness score (0.0-1.0)", ge=0.0, le=1.0)
    quality_score: float = Field(description="Quality score (0.0-1.0)", ge=0.0, le=1.0)
    strengths: List[str] = Field(description="List of strengths identified")
    weaknesses: List[str] = Field(description="List of weaknesses identified")
    critical_issues: List[str] = Field(description="Critical issues that need attention")
    improvement_suggestions: List[str] = Field(description="Specific improvement suggestions")


class ComprehensiveJudgment(BaseModel):
    """Comprehensive judgment of all agent outputs."""
    overall_assessment: str = Field(description="Overall assessment of all agents")
    agent_reviews: List[AgentOutputReview] = Field(description="Individual reviews for each agent")
    common_issues: List[str] = Field(description="Issues common across multiple agents")
    best_practices_identified: List[str] = Field(description="Best practices observed")
    priority_improvements: List[str] = Field(description="High-priority improvements recommended")
    overall_score: float = Field(description="Overall score across all agents (0.0-1.0)", ge=0.0, le=1.0)


# ============================================================================
# JUDGE AGENT
# ============================================================================

judge_agent = Agent(
    judge_model,
    model_settings=ModelSettings(max_output_tokens=4000, temperature=0.3),
    system_prompt="""Vous Ãªtes un expert en Ã©valuation critique de systÃ¨mes d'IA financiers. 
Votre rÃ´le est d'analyser de maniÃ¨re rigoureuse les sorties des agents financiers et de fournir des jugements critiques constructifs.

CRITÃˆRES D'Ã‰VALUATION:
1. **Correctness (Exactitude)**: Les rÃ©sultats sont-ils mathÃ©matiquement corrects? Les calculs sont-ils prÃ©cis? VÃ©rifiez les calculs arithmÃ©tiques (ex: sommes, multiplications).
2. **Quality (QualitÃ©)**: La sortie est-elle bien structurÃ©e? Les donnÃ©es sont-elles complÃ¨tes et cohÃ©rentes? Le format de rÃ©ponse est-il standardisÃ©?
3. **Tool Usage (Utilisation d'outils)**: 
   - Les outils ont-ils Ã©tÃ© utilisÃ©s correctement? Y a-t-il des calculs manuels non autorisÃ©s?
   - Y a-t-il des appels d'outils dupliquÃ©s? Comptez les appels uniques vs total des appels.
   - Les outils requis ont-ils Ã©tÃ© appelÃ©s (ex: calculer_rendement_portfolio pour Agent 3)?
4. **Input Validation (Validation des entrÃ©es)**: Les donnÃ©es d'entrÃ©e ont-elles Ã©tÃ© validÃ©es? Y a-t-il des vÃ©rifications de format, de plage, ou de champs requis?
5. **Response Format Consistency (CohÃ©rence du format)**: Les formats de rÃ©ponse sont-ils cohÃ©rents entre les agents? Les structures de donnÃ©es sont-elles standardisÃ©es?
6. **Compliance (ConformitÃ©)**: Les rÃ¨gles de conformitÃ© sont-elles respectÃ©es?
7. **Best Practices (Meilleures pratiques)**: Les meilleures pratiques sont-elles suivies?

STRUCTURE DE VOTRE Ã‰VALUATION:
- Identifiez les forces de chaque agent
- Identifiez les faiblesses et problÃ¨mes critiques
- Fournissez des suggestions d'amÃ©lioration concrÃ¨tes et actionnables
- Identifiez les problÃ¨mes communs Ã  plusieurs agents
- Recommandez des amÃ©liorations prioritaires

SOYEZ CRITIQUE MAIS CONSTRUCTIF:
- Ne soyez pas complaisant - identifiez les vrais problÃ¨mes
- Soyez spÃ©cifique dans vos critiques
- Proposez des solutions pratiques
- Priorisez les amÃ©liorations par impact

RÃ©pondez avec un objet ComprehensiveJudgment structurÃ©.""",
    output_type=ComprehensiveJudgment,
)


# ============================================================================
# EVALUATION FUNCTIONS
# ============================================================================

async def judge_agent_output(
    agent_name: str,
    agent_output: Any,
    expected_result: Optional[Any] = None,
    tool_calls: Optional[List[Dict[str, Any]]] = None,
    input_prompt: Optional[str] = None,
) -> AgentOutputReview:
    """Judge a single agent's output.
    
    Args:
        agent_name: Name of the agent (e.g., "Agent 1", "Agent 2")
        agent_output: The output from the agent
        expected_result: Expected result for correctness comparison
        tool_calls: List of tool calls made by the agent
        input_prompt: Original input prompt to the agent
        
    Returns:
        AgentOutputReview with critical analysis
    """
    # Build evaluation prompt
    evaluation_prompt = f"""Ã‰valuez la sortie de {agent_name}.

SORTIE DE L'AGENT:
{str(agent_output)}

"""
    
    if input_prompt:
        evaluation_prompt += f"PROMPT ORIGINAL:\n{input_prompt}\n\n"
    
    if expected_result:
        evaluation_prompt += f"RÃ‰SULTAT ATTENDU:\n{expected_result}\n\n"
    
    if tool_calls:
        evaluation_prompt += f"APPELS D'OUTILS:\n"
        for tc in tool_calls:
            evaluation_prompt += f"- {tc}\n"
        evaluation_prompt += "\n"
    
    evaluation_prompt += """Analysez cette sortie de maniÃ¨re critique:
1. VÃ©rifiez l'exactitude des calculs et rÃ©sultats (vÃ©rifiez les calculs arithmÃ©tiques)
2. Ã‰valuez la qualitÃ© de la structure et des donnÃ©es (format standardisÃ©?)
3. VÃ©rifiez l'utilisation correcte des outils (outils requis appelÃ©s? appels dupliquÃ©s?)
4. VÃ©rifiez la validation des entrÃ©es (donnÃ©es validÃ©es avant traitement?)
5. VÃ©rifiez la cohÃ©rence du format de rÃ©ponse (structure standardisÃ©e?)
6. Identifiez les forces et faiblesses
7. Proposez des amÃ©liorations concrÃ¨tes"""

    result = await judge_agent.run(evaluation_prompt)
    judgment = result.output
    
    # Extract review for this specific agent
    if judgment.agent_reviews:
        for review in judgment.agent_reviews:
            if review.agent_name == agent_name:
                return review
    
    # Fallback: create review from overall judgment
    return AgentOutputReview(
        agent_name=agent_name,
        correctness_score=judgment.overall_score,
        quality_score=judgment.overall_score,
        strengths=judgment.best_practices_identified,
        weaknesses=judgment.common_issues,
        critical_issues=judgment.priority_improvements[:3],
        improvement_suggestions=judgment.priority_improvements,
    )


async def judge_all_agents(
    agent_outputs: Dict[str, Dict[str, Any]],
) -> ComprehensiveJudgment:
    """Judge outputs from all agents (1-5).
    
    Args:
        agent_outputs: Dictionary mapping agent names to their outputs
        Format: {
            "Agent 1": {
                "output": ...,
                "expected_result": ...,
                "tool_calls": [...],
                "input_prompt": "..."
            },
            ...
        }
        
    Returns:
        ComprehensiveJudgment with reviews of all agents
    """
    # Build comprehensive evaluation prompt
    evaluation_prompt = """Ã‰valuez de maniÃ¨re critique les sorties de tous les agents financiers (Agent 1 Ã  Agent 5).

"""
    
    for agent_name, agent_data in agent_outputs.items():
        evaluation_prompt += f"=== {agent_name} ===\n"
        
        if agent_data.get("input_prompt"):
            evaluation_prompt += f"PROMPT: {agent_data['input_prompt']}\n"
        
        evaluation_prompt += f"SORTIE:\n{str(agent_data.get('output', 'N/A'))}\n"
        
        if agent_data.get("expected_result"):
            evaluation_prompt += f"ATTENDU: {agent_data['expected_result']}\n"
        
        if agent_data.get("tool_calls"):
            evaluation_prompt += f"OUTILS UTILISÃ‰S: {', '.join(str(tc) for tc in agent_data['tool_calls'][:3])}\n"
        
        evaluation_prompt += "\n"
    
    evaluation_prompt += """Analysez TOUS ces agents de maniÃ¨re critique et complÃ¨te:
1. Ã‰valuez chaque agent individuellement:
   - Correctness: VÃ©rifiez les calculs arithmÃ©tiques (ex: sommes, multiplications)
   - Quality: Structure, complÃ©tude, format standardisÃ©
   - Tool Usage: Outils requis appelÃ©s? Appels dupliquÃ©s? (comptez appels uniques vs total)
   - Input Validation: DonnÃ©es validÃ©es avant traitement?
   - Response Format: Format cohÃ©rent et standardisÃ©?
2. Identifiez les problÃ¨mes communs Ã  plusieurs agents (ex: appels dupliquÃ©s, absence de validation, formats non standardisÃ©s)
3. Identifiez les meilleures pratiques observÃ©es
4. Recommandez des amÃ©liorations prioritaires pour chaque agent
5. Fournissez un score global et une Ã©valuation d'ensemble

Soyez rigoureux, critique mais constructif. Identifiez les vrais problÃ¨mes et proposez des solutions pratiques."""

    result = await judge_agent.run(evaluation_prompt)
    return result.output


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

async def judge_evaluation_results(evaluation_results_file: str = "examples/evaluate_all_agents_results.json"):
    """Judge agent outputs from evaluation results file.
    
    Args:
        evaluation_results_file: Path to JSON file with evaluation results
        
    Returns:
        ComprehensiveJudgment of all agents
    """
    import json
    from pathlib import Path
    
    results_path = Path(evaluation_results_file)
    if not results_path.exists():
        raise FileNotFoundError(f"Evaluation results file not found: {evaluation_results_file}")
    
    with open(results_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # Build agent outputs dictionary from evaluation results
    agent_outputs = {}
    for agent_data in data.get("agents", []):
        agent_name = agent_data.get("name", "Unknown Agent")
        agent_outputs[agent_name] = {
            "output": agent_data.get("output_text", ""),
            "expected_result": agent_data.get("expected_result"),
            "actual_result": agent_data.get("actual_result"),
            "input_prompt": agent_data.get("input_prompt", ""),
            "tool_calls": agent_data.get("tool_calls", []),
            "correctness": agent_data.get("correctness", ""),
            "errors": agent_data.get("errors", []),
        }
    
    return await judge_all_agents(agent_outputs)


async def exemple_judgment():
    """Example of using the judge agent."""
    print("âš–ï¸  Judge Agent: Critical Evaluation of Agent Outputs")
    print("=" * 70)
    
    # Example: Judge Agent 1 output
    agent_1_output = {
        "positions": [
            {"symbole": "AIR.PA", "quantite": 50, "prix_achat": 120.0, "date_achat": "2024-01-15"},
            {"symbole": "SAN.PA", "quantite": 30, "prix_achat": 85.0, "date_achat": "2024-02-20"},
        ],
        "valeur_totale": 8550.0,
        "date_evaluation": "2024-03-01"
    }
    
    print("\n1. Evaluating Agent 1 output...")
    review = await judge_agent_output(
        agent_name="Agent 1",
        agent_output=agent_1_output,
        expected_result=8550.0,
        input_prompt="Extrais le portfolio: 50 AIR.PA Ã  120â‚¬, 30 SAN.PA Ã  85â‚¬",
    )
    
    print(f"\nâœ… Agent: {review.agent_name}")
    print(f"   Correctness Score: {review.correctness_score:.2f}/1.0")
    print(f"   Quality Score: {review.quality_score:.2f}/1.0")
    print(f"\n   Forces:")
    for strength in review.strengths:
        print(f"     + {strength}")
    print(f"\n   Faiblesses:")
    for weakness in review.weaknesses:
        print(f"     - {weakness}")
    print(f"\n   ProblÃ¨mes Critiques:")
    for issue in review.critical_issues:
        print(f"     âš ï¸  {issue}")
    print(f"\n   Suggestions d'AmÃ©lioration:")
    for suggestion in review.improvement_suggestions:
        print(f"     ðŸ’¡ {suggestion}")
    
    # Example: Comprehensive judgment of multiple agents
    print("\n" + "=" * 70)
    print("2. Comprehensive Evaluation of All Agents...")
    
    all_outputs = {
        "Agent 1": {
            "output": agent_1_output,
            "expected_result": 8550.0,
            "input_prompt": "Extrais le portfolio: 50 AIR.PA Ã  120â‚¬, 30 SAN.PA Ã  85â‚¬",
            "tool_calls": [],
        },
        "Agent 2": {
            "output": "Valeur future: 74,012.21â‚¬",
            "expected_result": 74012.21,
            "input_prompt": "50,000â‚¬ at 4% for 10 years. Future value?",
            "tool_calls": [{"name": "calculer_valeur_future", "args": {"capital_initial": 50000, "taux": 0.04, "duree": 10}}],
        },
    }
    
    comprehensive = await judge_all_agents(all_outputs)
    
    print(f"\nðŸ“Š Ã‰valuation Globale:")
    print(f"   Score Global: {comprehensive.overall_score:.2f}/1.0")
    print(f"\n   Ã‰valuation d'Ensemble:")
    print(f"   {comprehensive.overall_assessment}")
    
    print(f"\n   ProblÃ¨mes Communs:")
    for issue in comprehensive.common_issues:
        print(f"     - {issue}")
    
    print(f"\n   Meilleures Pratiques IdentifiÃ©es:")
    for practice in comprehensive.best_practices_identified:
        print(f"     âœ“ {practice}")
    
    print(f"\n   AmÃ©liorations Prioritaires:")
    for improvement in comprehensive.priority_improvements:
        print(f"     ðŸ”§ {improvement}")


async def exemple_judge_from_evaluation_file():
    """Example: Judge agents using evaluation results file."""
    print("âš–ï¸  Judge Agent: Evaluating from evaluation_results.json")
    print("=" * 70)
    
    try:
        comprehensive = await judge_evaluation_results()
        
        print(f"\nðŸ“Š Ã‰valuation Globale:")
        print(f"   Score Global: {comprehensive.overall_score:.2f}/1.0")
        print(f"\n   Ã‰valuation d'Ensemble:")
        print(f"   {comprehensive.overall_assessment[:500]}...")
        
        print(f"\n   ProblÃ¨mes Communs:")
        for issue in comprehensive.common_issues[:5]:
            print(f"     - {issue}")
        
        print(f"\n   Meilleures Pratiques IdentifiÃ©es:")
        for practice in comprehensive.best_practices_identified[:5]:
            print(f"     âœ“ {practice}")
        
        print(f"\n   AmÃ©liorations Prioritaires:")
        for improvement in comprehensive.priority_improvements[:5]:
            print(f"     ðŸ”§ {improvement}")
        
        print(f"\n   DÃ©tails par Agent:")
        for review in comprehensive.agent_reviews:
            print(f"\n   {review.agent_name}:")
            print(f"     Correctness: {review.correctness_score:.2f}/1.0")
            print(f"     Quality: {review.quality_score:.2f}/1.0")
            if review.critical_issues:
                print(f"     âš ï¸  ProblÃ¨mes: {', '.join(review.critical_issues[:2])}")
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("   Run evaluate_all_agents.py first to generate evaluation results.")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--from-file":
        # Judge from evaluation results file
        asyncio.run(exemple_judge_from_evaluation_file())
    else:
        # Standard example
        asyncio.run(exemple_judgment())

